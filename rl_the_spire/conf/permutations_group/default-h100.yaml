dataset:
  n_max_permutation_size: 5
  gamma: 2.0
  batch_size: 256  # Increased batch size for GPU
  num_workers: 8   # Increased number of workers
  prefetch_factor: 4  # Increased prefetch factor
encoder:
  n_embed: 128
  n_heads: 4
  n_layers: 6
  attn_dropout: 0.0
  resid_dropout: 0.0
  dtype: "float16"  # Using mixed precision for better performance
  device: "cuda"   # Changed from CPU to CUDA
  init_std: 0.02
  mlp_dropout: 0.0
  ln_eps: 1e-5
  n_output_heads: 2
  n_output_embed: 16
  n_output_rows: 8
  n_output_columns: 8
  activation: "gelu"
  linear_size_multiplier: 4
vae:
  gamma: 0.5
  kl_loss_weight: 0.01
conv_transformer:
  n_heads: 4
inverter_network:
  n_layers: 3
composer_network:
  n_layers: 3
iterations: 10000
eval_interval: 100
experiment_name: "permutations_group_h100_gpu"  # Updated experiment name
wandb_enabled: true
log_interval: 20
reconstruction_loss_weight: 1.0
neural_inv_perm_loss_weight: 1.0
neural_comp_perm_loss_weight: 1.0 