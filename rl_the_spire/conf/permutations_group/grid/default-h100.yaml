dataset:
  n_max_permutation_size: 15  # Increased with respect to default.yaml, GPUs can roar
  gamma: 2.0
  batch_size: 128
  num_workers: 2
  prefetch_factor: 2
encoder:
  n_embed: 256
  n_heads: 8
  n_layers: 8
  encoder_attn_dropout: 0.1
  encoder_resid_dropout: 0.1
  encoder_mlp_dropout: 0.1
  dtype: "float32"
  device: "cuda"
  init_std: 0.02
  ln_eps: 1e-5
  n_output_heads: 4
  n_output_embed: 128
  n_output_rows: 6
  n_output_columns: 6
  activation: "gelu"
  linear_size_multiplier: 4
  sigma_output: false
  conv_blocks: 2
optimizer:
  lr: 1.0e-4
  warmup_steps: 5000
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.01
vae:
  kl_loss_weight: 0.001
  kl_warmup_steps: 10000
  kl_warmup_start_weight: 0.0
  # Gamma parameters
  gamma_warmup_steps: 15000
  gamma_start: 1.0  # Start with high value (less noise)
  gamma_final: 0.5  # Final gamma value (more exploration)
conv_transformer:
  n_heads: 4
  denoiser_blocks: 3
inverter_network:
  n_layers: 4
composer_network:
  n_layers: 6
# General dropout parameters for other components
attn_dropout: 0.0
resid_dropout: 0.0
mlp_dropout: 0.0
num_live_to_target_adapter_layers: 2
iterations: 100000
eval_interval: 100
experiment_name: "permutations_group_grid_vae_h100_gpu"  # Updated experiment name
wandb_enabled: true
log_interval: 100
reconstruction_loss_weight: 1.0
neural_inv_perm_loss_weight: 0.5
neural_comp_perm_loss_weight: 0.5
latent_inv_perm_loss_weight: 0.25
latent_comp_perm_loss_weight: 0.25
latent_sampled_perm_loss_weight: 0.50
# Latent loss warmup params
latent_warmup_steps: 15000
latent_warmup_start_weight: 0.00
latent_warmup_delay_steps: 0
group_operations_warmup_steps: 100
use_ema_target: true
# EMA tau scheduling
ema_tau_final: 0.005
ema_tau_start: 0.5
ema_tau_warmup_steps: 5000
ema_tau: ${ema_tau_final}
init_ema_target_as_zeros: false
