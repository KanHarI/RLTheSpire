dataset:
  n_max_permutation_size: 5
  gamma: 2.0
  batch_size: 128
  num_workers: 2
  prefetch_factor: 2
encoder:
  n_embed: 128
  n_heads: 4
  n_layers: 6
  attn_dropout: 0.0
  resid_dropout: 0.0
  dtype: "float32"
  device: "cpu"
  init_std: 0.02
  mlp_dropout: 0.0
  ln_eps: 1e-5
  n_output_heads: 2
  n_output_embed: 16
  n_output_rows: 8
  n_output_columns: 8
  activation: "gelu"
  linear_size_multiplier: 4
  sigma_output: false
  conv_blocks: 1
optimizer:
  lr: 2.0e-4
  warmup_steps: 100
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.01
vae:
  gamma: 0.5
  kl_loss_weight: 0.01
  kl_warmup_steps: 500
  kl_warmup_start_weight: 0.0
conv_transformer:
  n_heads: 4
  denoiser_blocks: 1
inverter_network:
  n_layers: 3
composer_network:
  n_layers: 3
num_live_to_target_adapter_layers: 1
iterations: 10000
eval_interval: 100
experiment_name: "permutations_group_grid_vae_cpu_default"
wandb_enabled: true
log_interval: 20
reconstruction_loss_weight: 1.0
neural_inv_perm_loss_weight: 0.5
neural_comp_perm_loss_weight: 0.5
latent_inv_perm_loss_weight: 0.01
latent_comp_perm_loss_weight: 0.01
latent_sampled_perm_loss_weight: 0.01
# Latent loss warmup params
latent_warmup_steps: 0
latent_warmup_start_weight: 0.0
latent_warmup_delay_steps: 0
use_ema_target: true
# EMA tau scheduling
ema_tau_final: 0.005  # Final tau value (same as current fixed value)
ema_tau_start: 0.05   # Initial higher tau value
ema_tau_warmup_steps: 5000  # Steps to decrease from start to final value
ema_tau: ${ema_tau_final}  # Keeping for backward compatibility
init_ema_target_as_zeros: false
